{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_file(input_path, query):\n",
    "    files_paths = glob.glob(input_path + query)\n",
    "    return files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_file(file_names, save_path, train=True):\n",
    "    for i in tqdm(range(len(file_names))):\n",
    "        img = cv2.imread(file_names[i])\n",
    "        img = cv2.resize(img, (384 ,384))\n",
    "        if train:\n",
    "            try:\n",
    "                if not(os.path.isdir(save_path + 'train/' + file_names[i].split('/')[-2])):\n",
    "                    os.makedirs(os.path.join(save_path + 'train/' + file_names[i].split('/')[-2]))\n",
    "            except OSError as e:\n",
    "                if e.errno != errno.EEXIST:\n",
    "                    print(\"Failed to create train directory!!!!!\")\n",
    "                    raise\n",
    "                    \n",
    "            if not(os.path.isfile(save_path + 'train/' + file_names[i].split('/')[-2] + '/' + os.path.splitext(file_names[i].split('/')[-1])[0] + '.jpg')):                    \n",
    "                    cv2.imwrite(os.path.join(save_path + 'train/' + file_names[i].split('/')[-2] + '/' + os.path.splitext(file_names[i].split('/')[-1])[0] + '.jpg'), img)\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = ['/*.JPG', '/*/*.JPG', '/*/*/*.JPG', '/*/*/*/*.JPG', '/*/*/*/*/*.JPG', '/*/*.jpg', '/*/*/*.jpg', '/*/*/*/*.jpg', '/*/*/*/*/*.jpg', '/*/*/*/*/*/*.jpg']\n",
    "file_path = \"/home/dacon/Dacon/HDD_02/extract_landmark_2nd\"\n",
    "save_path = \"/home/dacon/Dacon/HDD_02/landmark-resize/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*/*.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131048/131048 [5:52:40<00:00,  6.19it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*/*/*.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*/*/*/*.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*/*/*/*/*.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*/*.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376882/376882 [12:05:58<00:00,  8.65it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*/*/*.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*/*/*/*.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*/*/*/*/*.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*/*/*/*/*/*.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for query in QUERY:\n",
    "    file_names = read_image_file(file_path, query)\n",
    "    \n",
    "    try:\n",
    "        cpu_n = os.cpu_count() - 6\n",
    "        pool = multiprocessing.Pool(processes=cpu_n)\n",
    "        pool.map(resize_image_file(file_names, save_path, train=True))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    except:\n",
    "        print(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl",
   "language": "python",
   "name": "nfl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
