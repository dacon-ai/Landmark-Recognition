{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dacon/Dacon/HDD_02/landmark/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_for_metric(df_answer, df_submission):\n",
    "    '''\n",
    "    Args | df_answer: Pandas DataFrame | df_submission: Pandas DataFrame\n",
    "    Return | true: Numpy Array | pred: Numpy Array\n",
    "    '''\n",
    "    df_1 = df_answer\n",
    "    df_2 = df_submission\n",
    "\n",
    "    id_column = df_1.columns[0]\n",
    "\n",
    "    df_1.index = df_1[id_column]\n",
    "\n",
    "    df_2.index = df_2[id_column]\n",
    "    df_2 = df_2.loc[df_1.index]\n",
    "    \n",
    "    return df_1, df_2\n",
    "\n",
    "def gap(true_df, pred_df):\n",
    "    true_df, pred_df = select_for_metric(true_df, pred_df)\n",
    "    \"\"\"\n",
    "    Compute Global Average Precision score (GAP)\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : Dict[Any, Any]\n",
    "        Dictionary with query ids and true ids for query samples\n",
    "    y_pred : Dict[Any, Tuple[Any, float]]\n",
    "        Dictionary with query ids and predictions (predicted id, confidence\n",
    "        level)\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        GAP score\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from evaluations.kaggle_2020 import global_average_precision_score\n",
    "    >>> y_true = {\n",
    "    ...         'id_001': 123,\n",
    "    ...         'id_002': None,\n",
    "    ...         'id_003': 999,\n",
    "    ...         'id_004': 123,\n",
    "    ...         'id_005': 999,\n",
    "    ...         'id_006': 888,\n",
    "    ...         'id_007': 666,\n",
    "    ...         'id_008': 666,\n",
    "    ...         'id_009': None,\n",
    "    ...         'id_010': 666,\n",
    "    ...     }\n",
    "    >>> y_pred = {\n",
    "    ...         'id_001': (123, 0.15),\n",
    "    ...         'id_002': (123, 0.10),\n",
    "    ...         'id_003': (999, 0.30),\n",
    "    ...         'id_005': (999, 0.40),\n",
    "    ...         'id_007': (555, 0.60),\n",
    "    ...         'id_008': (666, 0.70),\n",
    "    ...         'id_010': (666, 0.99),\n",
    "    ...     }\n",
    "    >>> global_average_precision_score(y_true, y_pred)\n",
    "    0.5479166666666666\n",
    "    >>> itâ€™s 1 if the i-th prediction is correct, and 0 otherwise\n",
    "    \"\"\"\n",
    "    y_pred = {}\n",
    "    for i, value in zip(pred_df['id'], pred_df[['landmark_id', 'conf']].values):\n",
    "        y_pred[i] = tuple(value)\n",
    "        \n",
    "    y_true = {}\n",
    "    for i, value in zip(true_df['id'], true_df[['landmark_id']].values):\n",
    "        y_true[i] = tuple(value)\n",
    "    indexes = list(y_pred.keys())\n",
    "    indexes.sort(\n",
    "        key=lambda x: -y_pred[x][1],\n",
    "    )\n",
    "    queries_with_target = len([i for i in y_true.values() if i[0] is not ''])\n",
    "    correct_predictions = 0\n",
    "    total_score = 0.\n",
    "    for i, k in enumerate(indexes, 1):\n",
    "        relevance_of_prediction_i = 0\n",
    "        if y_true[k] == y_pred[k][0]:\n",
    "            correct_predictions += 1\n",
    "            relevance_of_prediction_i = 1\n",
    "        precision_at_rank_i = correct_predictions / i\n",
    "        total_score += precision_at_rank_i * relevance_of_prediction_i\n",
    "    return 1 / queries_with_target * total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../data/train/'\n",
    "query = '*/*.jpg'\n",
    "file_name = '../data/train_labels_0.csv'\n",
    "train_df, mapping = read_train_file(input_path, file_name, query)\n",
    "\n",
    "gt_df = pd.read_csv('../data/test_labels_0.csv')\n",
    "pred_df = pd.read_csv('../output/submission.csv')\n",
    "\n",
    "y_true = {}\n",
    "for i, value in zip(gt_df['id'], gt_df[['landmark_id']].values):\n",
    "    y_true[i] = tuple(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5372615890114104e-08"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap(gt_df, pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landmark",
   "language": "python",
   "name": "landmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
